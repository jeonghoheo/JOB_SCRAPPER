# BLUEPRINT | DONT EDIT

from bs4 import BeautifulSoup
import requests

MAIN_URL = "https://berlinstartupjobs.com"
ENGINEERING_URL = f"{MAIN_URL}/engineering/"
SKILL_AREA_URL = f"{MAIN_URL}/skill-areas/"
SKILL_AREA_PYTHON_URL = f"{SKILL_AREA_URL}python/"
SKILL_AREA_JAVASCRIPT_URL = f"{SKILL_AREA_URL}javascript/"
SKILL_AREA_TYPESCRIPT_URL = f"{SKILL_AREA_URL}typescript/"

# /BLUEPRINT

# ğŸ‘‡ğŸ» YOUR CODE ğŸ‘‡ğŸ»:

# í—¤ë”ë¥¼ ì‹¤ì œ ë¸Œë¼ìš°ì €ì˜ ê²ƒê³¼ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ì¶”ê°€í•´ ì¤ë‹ˆë‹¤.(ë´‡ìœ¼ë¡œ ì¸ì‹í•˜ëŠ”ê²ƒì„ ë°©ì§€í•˜ê¸°ìœ„í•´)
# Add headers to mimic a real browser to prevent being blocked as a bot.
# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€ 
# (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ĞºĞ°Ğº Ğ±Ğ¾Ñ‚Ğ°).

headers = {
	'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}


def scrap_url(url, area):
    print("***************************")
    print(f"AREA: {area}")
    print(f"Scrapping {url}...")
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")
    jobs = soup.find_all("li", class_="bjs-jlid")[0:-1]
    for job in jobs:
        job_title = job.find("h4", class_="bjs-jlid__h").text
        company_name = job.find("a", class_="bjs-jlid__b").text
        job_description = job.find("div", class_="bjs-jlid__description").text
        job_link = job.find("h4", class_="bjs-jlid__h").find("a").attrs.get("href")
        job_info = {
            "job_title":job_title,
            "company_name":company_name,
            "job_description":job_description,
            "job_ink":job_link
        }
        print("==============")
        print(f"JOB INFORMATION: {job_info}")    
        
def get_pages():
    response = requests.get(ENGINEERING_URL, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")
    # íŠ¹ì • íƒœê·¸ë¡œ ì •í•´ì ¸ ìˆì§€ ì•Šì„ë•Œ selectë¡œ íŠ¹ì • classëª…ìœ¼ë¡œë§Œ ê²€ìƒ‰í•´ì„œ í•´ë‹¹ íƒœê·¸ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
    # When the tag name isn't specific, use select with a class name pattern 
    # to get all matching tags.
    # ĞšĞ¾Ğ³Ğ´Ğ° Ğ¸Ğ¼Ñ Ñ‚ĞµĞ³Ğ° Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ select Ñ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ¸Ğ¼ĞµĞ½Ğ¸ ĞºĞ»Ğ°ÑÑĞ° 
    # Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ÑĞµÑ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµĞ³Ğ¾Ğ².
    buttons_len = len(soup.select('[class*="page-numbers"]'))
    return buttons_len - 1

for page in range(get_pages()):
    url = f"{ENGINEERING_URL}page/{page + 1}"
    scrap_url(url, "ENGINEERING_URL")

# ê° ìŠ¤í‚¬ë³„ ì§ì—…ë“¤ ëª©ë¡ì„ ìŠ¤í¬ë©
# Now, scrape jobs for specific skill areas
# Ğ¢ĞµĞ¿ĞµÑ€ÑŒ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ²
# PYTHON
scrap_url(SKILL_AREA_PYTHON_URL, "SKILL_AREA_PYTHON_URL")
# JAVASCRIPT
scrap_url(SKILL_AREA_JAVASCRIPT_URL, "SKILL_AREA_JAVASCRIPT_URL")
# TYPESCRIPT
scrap_url(SKILL_AREA_TYPESCRIPT_URL, "SKILL_AREA_TYPESCRIPT_URL")

# /YOUR CODE